üè† Precios de la vivienda en California: An√°lisis de macrodatos
Pit√≥n
PySpark
Google Colab
Licencia

Un pipeline completo de Big Data para predicci√≥n de precios de viviendas en California utilizando PySpark, optimizaciones distribuidas y machine learning escalable.

üìä Demostraci√≥n R√°pida en Google Colab
Abrir en Colab

```bash

Ejecuci√≥n inmediata en Colab
!pip install pyspark findspark
!git clone https://github.com/tu-usuario/california-housing-bigdata.git
%cd california-housing-bigdata

from src.main_pipeline import HousingPricePipeline
pipeline = HousingPricePipeline()
results = pipeline.run_complete_pipeline()

üéØ Descripci√≥n del Proyecto
Este proyecto implementa un pipeline de Big Data completo para predecir precios de viviendas en California utilizando el conjunto de datos p√∫blico de Kaggle. El sistema demuestra mejores pr√°cticas en procesamiento distribuido, optimizaci√≥n de rendimiento y aprendizaje autom√°tico escalable con PySpark.

üöÄ Caracter√≠sticas Principales
üì• Ingesta Multi-fuente: Datos desde Kaggle API, URLs p√∫blicas y datos de ejemplo

üßπ Procesamiento Robusto: Limpieza autom√°tica, validaci√≥n de calidad y transformaciones

‚ö° Optimizaciones Avanzadas: Caching estrat√©gico, particionamiento, configuraci√≥n Spark optimizada

ü§ñ ML Distribuido: Random Forest con PySpark ML e ingenier√≠a de funciones

üíæ Almacenamiento Eficiente: Parquet comprimido con particionamiento inteligente

üìä Visualizaci√≥n Integral: An√°lisis exploratorio autom√°tico y m√©tricas de rendimiento

üèóÔ∏è Arquitectura del Oleoducto
Diagrama de Flujo

https://imgur.com/a/BP5wmym

Componentes principales
DataIngestion: Descarga y carga de conjuntos de datos desde M√∫ltiples fuentes con resiliencia

Procesador de datos: Limpieza, validaci√≥n y transformaciones con manejo de valores nulos

FeatureIngenier√≠a: Creaci√≥n de caracter√≠sticas derivadas y codificaci√≥n categ√≥rica

OptimizationManager: T√©cnicas de optimizaci√≥n distribuida (caching, particionamiento)

ModelTrainer: Entrenamiento de modelos de ML con PySpark ML

DataStorage: Almacenamiento eficiente en m√∫ltiples formatos con compresi√≥n

üìÅ Estructura del Repositorio
california-housing-bigdata/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md # Este archivo
‚îú‚îÄ‚îÄ üìã requisitos.txt # Dependencias del proyecto
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/
‚îÇ ‚îî‚îÄ‚îÄ pipeline config.yaml # Configuraciones del pipeline
‚îÇ
‚îú‚îÄ‚îÄ üêç src/ # C√≥digo fuente principal
‚îÇ ‚îú‚îÄ‚îÄ init .py
‚îÇ ‚îú‚îÄ‚îÄ data ingestion.py # M√≥dulo de ingesta de datos
‚îÇ ‚îú‚îÄ‚îÄ data_processing.py # Procesamiento y limpieza
‚îÇ ‚îú‚îÄ‚îÄ feature_engineering.py # Ingenier√≠a de caracter√≠sticas
‚îÇ ‚îú‚îÄ‚îÄ data_storage.py # Almacenamiento optimizado
‚îÇ ‚îú‚îÄ‚îÄ optimizaci√≥n.py # T√©cnicas de optimizaci√≥n
‚îÇ ‚îú‚îÄ‚îÄ model_training.py # Entrenamiento de modelos
‚îÇ ‚îî‚îÄ‚îÄ main_pipeline.py # Pipeline principal unificado
‚îÇ
‚îú‚îÄ‚îÄ üìì notebooks/ # Jupyter notebooks
‚îÇ ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb # An√°lisis exploratorio de datos
‚îÇ ‚îú‚îÄ‚îÄ 02_feature_analysis.ipynb # An√°lisis de caracter√≠sticas
‚îÇ ‚îú‚îÄ‚îÄ 03_model_evaluaci√≥n.ipynb # Evaluaci√≥n de modelos
‚îÇ ‚îî‚îÄ‚îÄ 04_pipeline_demo.ipynb # Demostraci√≥n completa en Colab
‚îÇ
‚îú‚îÄ‚îÄ üíæ data/ # Datasets
‚îÇ ‚îú‚îÄ‚îÄ raw/ # Datos crudos
‚îÇ ‚îú‚îÄ‚îÄ procesados/ # Datos procesados
‚Äã‚Äã‚îÇ ‚îî‚îÄ‚îÄ models/ # Modelos entrenados
‚îÇ
‚îú‚îÄ‚îÄ ‚úÖ tests/ # Tests automatizados
‚îÇ ‚îú‚îÄ‚îÄ __init .py
‚îÇ ‚îú‚îÄ‚îÄ test_data_processing.py # Pruebas de procesamiento
‚îÇ ‚îú‚îÄ‚îÄ test_feature_engineering.py # Pruebas de ingenier√≠a
‚îÇ ‚îî‚îÄ‚îÄ test_optimization.py # Pruebas de optimizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üìä docs/ #Documentaci√≥n
‚îÇ ‚îú‚îÄ‚îÄ Architecture_diagrams/ # Diagramas de arquitectura
‚îÇ ‚îú‚îÄ‚îÄtechnical_report.pdf # Informe t√©cnico completo
‚îÇ ‚îî‚îÄ‚îÄ api_documentation.md # Documentaci√≥n de API
‚îÇ
‚îî‚îÄ‚îÄ üîß scripts/ # Scripts de utilidad
‚îú‚îÄ‚îÄ setup_environment.sh # Configuraci√≥n de entorno
‚îú‚îÄ‚îÄ run_pipeline.py # Ejecuci√≥n del pipeline
‚îî‚îÄ‚îÄ benchmark_performance.py # Benchmark de rendimiento

üöÄ Instalaci√≥n r√°pida
Opci√≥n 1: Google Colab (Recomendado)
Instalaci√≥n en Google Colab - Ejecutar en una celda
!pip install pyspark==3.4.0 findspark pandas matplotlib seaborn requests
!git clone https://github.com/tu-usuario/california-housing-bigdata.git
%cd california-housing-bigdata

Importante y ejecutar
import findspark
findspark.init()

from src.main_pipeline import HousingPricePipeline
pipeline = HousingPricePipeline()
results = pipeline.run_complete_pipeline()

Opci√≥n 2: Entorno Local
1. Clonar repositorio
git clone https://github.com/tu-usuario/california-housing-bigdata.git
cd california-housing-bigdata

2. Crear un entorno virtual (opcional pero recomendado)
python -m venv housing_env
source housing_env/bin/activate # Linux/Mac

housing_env\Scripts\activate # Windows
3. Instalar dependencias
pip install -r requirements.txt

4. Verificar instalaci√≥n
python -c ‚Äúfrom src.main_pipeline import HousingPricePipeline; print('‚úÖ Instalaci√≥n exitosa')‚Äù

Prerrequisitos
Python 3.8+
Java 8/11 (requerido para PySpark)
4 GB+ de RAM recomendados para un procesamiento eficiente
Google Colab o entorno local con las dependencias instaladas
üíª Uso del Pipeline
Ejecuci√≥n Completa Autom√°tica
from src.main_pipeline import HousingPricePipeline

Pipeline completo autom√°tico
pipeline = HousingPricePipeline()
datos_finales, modelo_entrenado, m√©tricas_de_rendimiento = pipeline.run_complete_pipeline()

Resultados generados autom√°ticamente
print(f‚Äù‚úÖ Pipeline completado: {final_data.count()} registros procesados‚Äù)
print(f‚Äùüìä M√©tricas: { Performance_metrics}‚Äù)

Ejecuci√≥n por M√≥dulos Individuales
Ingesta espec√≠fica
from src.data_ingestion import DataIngestion
ingestion = DataIngestion()
raw_data = ingestion.download_kaggle_dataset()

Procesamiento personalizado
from src.data_processing import DataProcessor
processor = DataProcessor(spark)
cleaned_data = processor.clean_data(raw_data)

Ingenier√≠a de caracter√≠sticas
datos_destacados = procesador.ingenier√≠a_de_caracter√≠sticas(datos_limpios)

Entrenamiento de modelo
de src.model_training importar ModelTrainer
entrenador = modelo ModelTrainer (spark)
, predicciones = entrenador.train_model (featured_data)

Scripts de L√≠nea de Comandos
Ejecutar pipeline completo
scripts de Python/run_pipeline.py

Evaluaci√≥n comparativa individual del rendimiento
scripts de Python/benchmark_performance.py

Ejecutar pruebas unitarias
python -m pytest tests/ -v

Ejecutar con configuraci√≥n personalizada
scripts de Python/run_pipeline.py ‚Äîconfig config/custom_config.yaml

‚ö° Optimizaciones implementadas
T√©cnicas de Performance
T√©cnica Mejora Impacto
Caching Estrat√©gico 35% Tiempo de procesamiento
Particionamiento Inteligente 40% Consultas filtradas
Configuraci√≥n Spark Adaptativa 25% Uso de memoria y CPU
Compresi√≥n Snappy 60% Almacenamiento en disco
Predicate Pushdown 30% Operaciones de filtrado

Configuraci√≥n optimizada de Spark
Configuraciones cr√≠ticas aplicadas
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skew.enabled", "true")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")
spark.conf.set("spark.sql.shuffle.partitions", "100")

üìä Resultados y M√©tricas
Rendimiento del modelo
M√©trica Valor Mejora vs Baseline
R¬≤ Puntuaci√≥n 0,81 +15%
RMSE $48.250 -22%
MAE $35.120 -18%
Tiempo Entrenamiento 28,7s -36,5%

Caracter√≠sticas m√°s importantes
ingreso_mediano (28,5%) - Factor m√°s predictivo

ocean_proximity_index (15.2%) - Ubicaci√≥n costera

latitud (12.8%) - Ubicaci√≥n geogr√°fica

habitaciones_por_hogar (9.5%) - Densidad habitacional

edad_mediana_vivienda (8.1%) - Antig√ºedad de viviendas

Benchmark de rendimiento
Operaci√≥n Original Optimizado Mejora
Conteo total 2.3s 1.4s 39.1%
Filtrar por precio 1.8s 0.9s 50.0%
Agrupar por ubicaci√≥n 3.2s 1.7s 46.9%
Entrenamiento de modelo 45.2s 28.7s 36.5%

üîß Configuraci√≥n avanzada
Archivo de configuraci√≥n principal
Edita config/pipeline_config.yaml:
Configuraci√≥n de Spark
spark_config:
app_name: ‚ÄúCaliforniaHousingPipeline‚Äù
executor_memory: ‚Äú2g‚Äù
driver_memory: ‚Äú1g‚Äù
sql_adaptive_enabled: true
shuffle_partitions: 100

Par√°metros del modelo
modelo:
algoritmo: ‚Äúrandom_forest‚Äù
par√°metros:
num_trees: 100
profundidad_m√°xima: 10
semilla: 42

Ingenier√≠a de caracter√≠sticas
ingenier√≠a_de_caracter√≠sticas:
caracter√≠sticas_derivadas:

- "rooms_per_household"
- "bedrooms_per_room"
- "population_per_household"
- "income_per_household"
Configuraci√≥n de almacenamiento
almacenamiento:
formato_primario: ‚Äúparquet‚Äù
compresi√≥n: ‚Äúsnappy‚Äù
columnas_de_partici√≥n: [‚Äúrango_de_precios‚Äù]

Variables de Entorno
Configurar para entorno local
export SPARK_HOME=/ruta/a/spark
export PYSPARK_PYTHON=python3
export JAVA_HOME=/ruta/a/java

Para Google Colab, se configura autom√°ticamente
üß™ Pruebas y Calidad de C√≥digo
Ejecuci√≥n de Tests
Pruebas unitarias
python -m pytest tests/ -v

Pruebas con cobertura
python -m pruebas pytest/ ‚Äîcov=src ‚Äîcov-report=html

Pruebas espec√≠ficas
python -m pytest tests/test_data_processing.py -v

Verificaci√≥n de Calidad
Formato de c√≥digo
negro src/ pruebas/

Pelusa
flake8 src/ pruebas/

Verificaci√≥n de tipos (opcional)
mypy src/

üìà Conjunto de datos y fuentes de datos
Precios de la vivienda en California
Fuente: Conjunto de datos de Kaggle

Registros: 20.640 propiedades

Caracter√≠sticas: 10 variables iniciales

Per√≠odo: Datos censales de California

Variables Principales
longitud, latitud: Coordenadas geogr√°ficas

housing_median_age: Edad media de las viviendas

total_habitaciones, total_dormitorios: Capacidad habitacional

poblaci√≥n, hogares: Datos demogr√°ficos

mediana_ingresos: Ingreso medio de hogares

median_house_value: Variable objetivo (precio)

ocean_proximity: Categor√≠a de ubicaci√≥n costera

ü§ù Contribuci√≥n
¬°Contribuciones son bienvenidas! Por favor sigue estos pasos:
Fork el proyecto

Crea una rama para tu feature (git checkout -b feature/AmazingFeature)

Confirma tus cambios (git commit -m 'Add AmazingFeature')

Push a la rama (git push origin feature/AmazingFeature)

Abre una solicitud de extracci√≥n

Gu√≠a de Desarrollo
1. Clonar y configurar
git clone https://github.com/tu-usuario/california-housing-bigdata.git
cd california-housing-bigdata

2. Instalar dependencias de desarrollo
pip install -r requirements-dev.txt

3. Configurar los ganchos de pre-commit
instalaci√≥n previa a la confirmaci√≥n

4. Desarrollar y testear
python -m pruebas pytest/ ‚Äîcov=src ‚Äîcov-report=html

Est√°ndares de C√≥digo
Seguir PEP 8 para c√≥digo Python

Usar docstrings para documentaci√≥n de funciones

Incluir pruebas para nuevas funcionalidades.

Mantener cobertura de c√≥digo > 80%

Actualizar la documentaci√≥n correspondiente

üêõ Soluci√≥n de Problemas
Problemas comunes
Error de memoria en Colab
Soluci√≥n: Reducir el tama√±o de datos o particiones
spark.conf.set("spark.sql.shuffle.partitions", "50")
spark.conf.set("spark.driver.memory", "1g")

Dependencias faltantes
Reinstalar dependencias
pip install --force-reinstall -r requirements.txt

Problema con Java
intento

Verificar instalaci√≥n de Java
java -versi√≥n

Depuraci√≥n
Habilitar logging detallado
import logging
logging.basicConfig(level=logging.INFO)

Verificar estad√≠sticas de datos
df.describe().show()
df.printSchema()

Monitorear uso de memoria
df.cache().count() Forzar el almacenamiento en cach√© y la memoria
