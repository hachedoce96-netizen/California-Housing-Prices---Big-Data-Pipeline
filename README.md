
# üè† California Housing Prices - Big Data Pipeline

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://python.org)
[![PySpark](https://img.shields.io/badge/PySpark-3.4.0-red)](https://spark.apache.org)
[![Google Colab](https://img.shields.io/badge/Google%20Colab-Compatible-orange)](https://colab.research.google.com)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

Un pipeline completo de Big Data para predicci√≥n de precios de viviendas en California utilizando PySpark, optimizaciones distribuidas y machine learning escalable.

## üìä Demo R√°pido en Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tu-usuario/california-housing-bigdata/blob/main/notebooks/04_pipeline_demo.ipynb)

```bash
# Ejecuci√≥n inmediata en Colab
!pip install pyspark findspark
!git clone https://github.com/tu-usuario/california-housing-bigdata.git
%cd california-housing-bigdata

from src.main_pipeline import HousingPricePipeline
pipeline = HousingPricePipeline()
results = pipeline.run_complete_pipeline()

# üéØ Descripci√≥n del Proyecto
Este proyecto implementa un pipeline de Big Data completo para predecir precios de viviendas en California utilizando el dataset p√∫blico de Kaggle. El sistema demuestra mejores pr√°cticas en procesamiento distribuido, optimizaci√≥n de performance y machine learning escalable con PySpark.

üöÄ Caracter√≠sticas Principales
üì• Ingesta Multi-fuente: Datos desde Kaggle API, URLs p√∫blicas y datos de ejemplo

üßπ Procesamiento Robusto: Limpieza autom√°tica, validaci√≥n de calidad y transformaciones

‚ö° Optimizaciones Avanzadas: Caching estrat√©gico, particionamiento, configuraci√≥n Spark optimizada

ü§ñ ML Distribuido: Random Forest con PySpark ML y feature engineering

üíæ Almacenamiento Eficiente: Parquet comprimido con particionamiento inteligente

üìä Visualizaci√≥n Integral: An√°lisis exploratorio autom√°tico y m√©tricas de performance


# üèóÔ∏è Arquitectura del Pipeline
Diagrama de Flujo

https://imgur.com/a/BP5wmym

## Componentes Principales
DataIngestion: Descarga y carga de datasets desde m√∫ltiples fuentes con resiliencia

DataProcessor: Limpieza, validaci√≥n y transformaciones con manejo de valores nulos

FeatureEngineering: Creaci√≥n de caracter√≠sticas derivadas y codificaci√≥n categ√≥rica

OptimizationManager: T√©cnicas de optimizaci√≥n distribuida (caching, particionamiento)

ModelTrainer: Entrenamiento de modelos de ML con PySpark ML

DataStorage: Almacenamiento eficiente en m√∫ltiples formatos con compresi√≥n
# üìÅ Estructura del Repositorio

california-housing-bigdata/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ README.md                         # Este archivo
‚îú‚îÄ‚îÄ üìã requirements.txt                  # Dependencias del proyecto
‚îú‚îÄ‚îÄ ‚öôÔ∏è config/
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_config.yaml             # Configuraciones del pipeline
‚îÇ
‚îú‚îÄ‚îÄ üêç src/                              # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion.py                # M√≥dulo de ingesta de datos
‚îÇ   ‚îú‚îÄ‚îÄ data_processing.py               # Procesamiento y limpieza
‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering.py           # Ingenier√≠a de caracter√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ data_storage.py                  # Almacenamiento optimizado
‚îÇ   ‚îú‚îÄ‚îÄ optimization.py                  # T√©cnicas de optimizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ model_training.py                # Entrenamiento de modelos
‚îÇ   ‚îî‚îÄ‚îÄ main_pipeline.py                 # Pipeline principal unificado
‚îÇ
‚îú‚îÄ‚îÄ üìì notebooks/                        # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb        # An√°lisis exploratorio de datos
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_analysis.ipynb        # An√°lisis de caracter√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ 03_model_evaluation.ipynb        # Evaluaci√≥n de modelos
‚îÇ   ‚îî‚îÄ‚îÄ 04_pipeline_demo.ipynb           # Demo completo en Colab
‚îÇ
‚îú‚îÄ‚îÄ üíæ data/                             # Datasets
‚îÇ   ‚îú‚îÄ‚îÄ raw/                             # Datos crudos
‚îÇ   ‚îú‚îÄ‚îÄ processed/                       # Datos procesados
‚îÇ   ‚îî‚îÄ‚îÄ models/                          # Modelos entrenados
‚îÇ
‚îú‚îÄ‚îÄ ‚úÖ tests/                            # Tests automatizados
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_data_processing.py          # Tests de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ test_feature_engineering.py      # Tests de ingenier√≠a
‚îÇ   ‚îî‚îÄ‚îÄ test_optimization.py             # Tests de optimizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üìä docs/                             # Documentaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ architecture_diagrams/           # Diagramas de arquitectura
‚îÇ   ‚îú‚îÄ‚îÄ technical_report.pdf             # Informe t√©cnico completo
‚îÇ   ‚îî‚îÄ‚îÄ api_documentation.md             # Documentaci√≥n de API
‚îÇ
‚îî‚îÄ‚îÄ üîß scripts/                          # Scripts de utilidad
    ‚îú‚îÄ‚îÄ setup_environment.sh             # Configuraci√≥n de entorno
    ‚îú‚îÄ‚îÄ run_pipeline.py                  # Ejecuci√≥n del pipeline
    ‚îî‚îÄ‚îÄ benchmark_performance.py         # Benchmark de rendimiento

#  üöÄ Instalaci√≥n R√°pida

##Opci√≥n 1: Google Colab (Recomendado)
# Instalaci√≥n en Google Colab - Ejecutar en una celda
!pip install pyspark==3.4.0 findspark pandas matplotlib seaborn requests
!git clone https://github.com/tu-usuario/california-housing-bigdata.git
%cd california-housing-bigdata

# Importar y ejecutar
import findspark
findspark.init()

from src.main_pipeline import HousingPricePipeline
pipeline = HousingPricePipeline()
results = pipeline.run_complete_pipeline()

# Opci√≥n 2: Entorno Local
# 1. Clonar repositorio
git clone https://github.com/tu-usuario/california-housing-bigdata.git
cd california-housing-bigdata

# 2. Crear entorno virtual (opcional pero recomendado)
python -m venv housing_env
source housing_env/bin/activate  # Linux/Mac
# housing_env\Scripts\activate  # Windows

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Verificar instalaci√≥n
python -c "from src.main_pipeline import HousingPricePipeline; print('‚úÖ Instalaci√≥n exitosa')"
# Prerrequisitos
### Python 3.8+
### Java 8/11 (requerido para PySpark)
### 4GB+ RAM recomendado para procesamiento eficiente
###Google Colab o entorno local con las dependencias instaladas
# üíª Uso del Pipeline
## Ejecuci√≥n Completa Autom√°tica
from src.main_pipeline import HousingPricePipeline

# Pipeline completo autom√°tico
pipeline = HousingPricePipeline()
final_data, trained_model, performance_metrics = pipeline.run_complete_pipeline()

# Resultados autom√°ticamente generados
print(f"‚úÖ Pipeline completado: {final_data.count()} registros procesados")
print(f"üìä M√©tricas: {performance_metrics}")
# Ejecuci√≥n por M√≥dulos Individuales
# Ingesta espec√≠fica
from src.data_ingestion import DataIngestion
ingestion = DataIngestion()
raw_data = ingestion.download_kaggle_dataset()

# Procesamiento personalizado
from src.data_processing import DataProcessor
processor = DataProcessor(spark)
cleaned_data = processor.clean_data(raw_data)

# Feature engineering avanzado
featured_data = processor.feature_engineering(cleaned_data)

# Entrenamiento de modelo
from src.model_training import ModelTrainer
trainer = ModelTrainer(spark)
model, predictions = trainer.train_model(featured_data)
# Scripts de L√≠nea de Comandos
# Ejecutar pipeline completo
python scripts/run_pipeline.py

# Solo benchmarking de performance
python scripts/benchmark_performance.py

# Ejecutar tests unitarios
python -m pytest tests/ -v

# Ejecutar con configuraci√≥n personalizada
python scripts/run_pipeline.py --config config/custom_config.yaml
# ‚ö° Optimizaciones Implementadas
## T√©cnicas de Performance

T√©cnica	Mejora	Impacto
Caching Estrat√©gico	35%	Tiempo de procesamiento
Particionamiento Inteligente	40%	Consultas filtradas
Configuraci√≥n Spark Adaptativa	25%	Uso de memoria y CPU
Compresi√≥n Snappy	60%	Almacenamiento en disco
Predicate Pushdown	30%	Operaciones de filtrado
# Configuraci√≥n Spark Optimizada
# Configuraciones cr√≠ticas aplicadas
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true") 
spark.conf.set("spark.sql.adaptive.skew.enabled", "true")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760")
spark.conf.set("spark.sql.shuffle.partitions", "100")
# üìä Resultados y M√©tricas
## Performance del Modelo
M√©trica	Valor	Mejora vs Baseline
R¬≤ Score	0.81	+15%
RMSE	$48,250	-22%
MAE	$35,120	-18%
Tiempo Entrenamiento	28.7s	-36.5%
# Caracter√≠sticas M√°s Importantes
median_income (28.5%) - Factor m√°s predictivo

ocean_proximity_index (15.2%) - Ubicaci√≥n costera

latitude (12.8%) - Ubicaci√≥n geogr√°fica

rooms_per_household (9.5%) - Densidad habitacional

housing_median_age (8.1%) - Antig√ºedad de viviendas
# Benchmark de Rendimiento
Operaci√≥n	Original	Optimizado	Mejora
Count total	2.3s	1.4s	39.1%
Filter por precio	1.8s	0.9s	50.0%
Group by ubicaci√≥n	3.2s	1.7s	46.9%
Model training	45.2s	28.7s	36.5%
# üîß Configuraci√≥n Avanzada
## Archivo de Configuraci√≥n Principal
### Editar config/pipeline_config.yaml:
# Spark Configuration
spark_config:
  app_name: "CaliforniaHousingPipeline"
  executor_memory: "2g"
  driver_memory: "1g"
  sql_adaptive_enabled: true
  shuffle_partitions: 100

# Model Parameters
model:
  algorithm: "random_forest"
  parameters:
    num_trees: 100
    max_depth: 10
    seed: 42

# Feature Engineering
feature_engineering:
  derived_features:
    - "rooms_per_household"
    - "bedrooms_per_room"
    - "population_per_household"
    - "income_per_household"

# Storage Settings
storage:
  primary_format: "parquet"
  compression: "snappy"
  partition_columns: ["price_range"]
#   Variables de Entorno
###### Configurar para entorno local
export SPARK_HOME=/path/to/spark
export PYSPARK_PYTHON=python3
export JAVA_HOME=/path/to/java

###### Para Google Colab, se configuran autom√°ticamente
# üß™ Testing y Calidad de C√≥digo
## Ejecuci√≥n de Tests
######Tests unitarios
python -m pytest tests/ -v

######  Tests con cobertura
python -m pytest tests/ --cov=src --cov-report=html

###### Tests espec√≠ficos
python -m pytest tests/test_data_processing.py -v
# Verificaci√≥n de Calidad
###### Formateo de c√≥digo
black src/ tests/

###### Linting
flake8 src/ tests/

###### Verificaci√≥n de tipos (opcional)
mypy src/
# üìà Dataset y Fuentes de Datos
## California Housing Prices
Fuente: Kaggle Dataset

Registros: 20,640 propiedades

Caracter√≠sticas: 10 variables iniciales

Per√≠odo: Datos censales de California
## Variables Principales
longitude, latitude: Coordenadas geogr√°ficas

housing_median_age: Edad media de las viviendas

total_rooms, total_bedrooms: Capacidad habitacional

population, households: Datos demogr√°ficos

median_income: Ingreso medio de hogares

median_house_value: Variable objetivo (precio)

ocean_proximity: Categ√≥rica de ubicaci√≥n costera
# ü§ù Contribuci√≥n
¬°Contribuciones son bienvenidas! Por favor sigue estos pasos:
Fork el proyecto

Crea una rama para tu feature (git checkout -b feature/AmazingFeature)

Commit tus cambios (git commit -m 'Add AmazingFeature')

Push a la rama (git push origin feature/AmazingFeature)

Abre un Pull Request
# Gu√≠a de Desarrollo
# 1. Clonar y configurar
git clone https://github.com/tu-usuario/california-housing-bigdata.git
cd california-housing-bigdata

###### 2. Instalar dependencias de desarrollo
pip install -r requirements-dev.txt

###### 3. Configurar pre-commit hooks
pre-commit install

###### 4. Desarrollar y testear
python -m pytest tests/ --cov=src --cov-report=html
## Est√°ndares de C√≥digo
Seguir PEP 8 para c√≥digo Python

Usar docstrings para documentaci√≥n de funciones

Incluir tests para nuevas funcionalidades

Mantener cobertura de c√≥digo > 80%

Actualizar documentaci√≥n correspondiente
# üêõ Soluci√≥n de Problemas
## Problemas Comunes
### Error de memoria en Colab

###### Soluci√≥n: Reducir tama√±o de datos o particiones
spark.conf.set("spark.sql.shuffle.partitions", "50")
spark.conf.set("spark.driver.memory", "1g")
### Dependencias faltantes
###### Reinstalar dependencias
pip install --force-reinstall -r requirements.txt
### Problemas con Java

bash
###### Verificar instalaci√≥n de Java
java -version
# Debugging
###### Habilitar logging detallado
import logging
logging.basicConfig(level=logging.INFO)

###### Verificar estad√≠sticas de datos
df.describe().show()
df.printSchema()

###### Monitorear uso de memoria
###### df.cache().count()  Forzar caching y ver memoria
